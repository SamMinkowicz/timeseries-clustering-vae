{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries clustering\n",
    "\n",
    "Time series clustering is to partition time series data into groups based on similarity or distance, so that time series in the same cluster are similar.\n",
    "\n",
    "Methodology followed:\n",
    "* Use Variational Recurrent AutoEncoder (VRAE) for dimensionality reduction of the timeseries\n",
    "* To visualize the clusters, PCA and t-sne are used\n",
    "\n",
    "Paper:\n",
    "https://arxiv.org/pdf/1412.6581.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contents\n",
    "\n",
    "0. [Load data and preprocess](#Load-data-and-preprocess)\n",
    "1. [Initialize VRAE object](#Initialize-VRAE-object)\n",
    "2. [Fit the model onto dataset](#Fit-the-model-onto-dataset)\n",
    "3. [Transform the input timeseries to encoded latent vectors](#Transform-the-input-timeseries-to-encoded-latent-vectors)\n",
    "4. [Save the model to be fetched later](#Save-the-model-to-be-fetched-later)\n",
    "5. [Visualize using PCA and tSNE](#Visualize-using-PCA-and-tSNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    "if (code_show){\n",
    "$('div.input').hide();\n",
    "} else {\n",
    "$('div.input').show();\n",
    "}\n",
    "code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vrae.vrae import VRAE\n",
    "from vrae.utils import *\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import plotly\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "plotly.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dload = './model_dir' #download directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 30\n",
    "hidden_size = 256\n",
    "hidden_layer_depth = 3\n",
    "latent_length = 64\n",
    "batch_size = 10\n",
    "learning_rate = 0.00002\n",
    "n_epochs = 10000\n",
    "dropout_rate = 0.0\n",
    "optimizer = 'Adam' # options: ADAM, SGD\n",
    "cuda = True # options: True, False\n",
    "print_every=30\n",
    "clip = True # options: True, False\n",
    "max_grad_norm=5\n",
    "loss = 'MSELoss' # options: SmoothL1Loss, MSELoss\n",
    "block = 'LSTM' # options: LSTM, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = open_data('data', ratio_train=1.0, dataset='ECG5000')\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "base = np.min(y_train)  # Check if data is 0-based\n",
    "if base != 0:\n",
    "    y_train -= base\n",
    "y_val -= base\n",
    "y_val = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data is sequential, so we need to reshape it to add a time dimension\n",
    "# X_train = X_train.reshape(X_train.shape[0], seq_len, -1)[:-6]\n",
    "\n",
    "# For learning embeddings, we don't care about overfitting\n",
    "X_val = X_train\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train))\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetch `sequence_length` from dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetch `number_of_features` from dataset**\n",
    "\n",
    "This config corresponds to number of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = X_train.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize VRAE object\n",
    "\n",
    "VRAE inherits from `sklearn.base.BaseEstimator` and overrides `fit`, `transform` and `fit_transform` functions, similar to sklearn modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vrae = VRAE(sequence_length=sequence_length,\n",
    "            number_of_features = number_of_features,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate,\n",
    "            optimizer = optimizer, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            loss = loss,\n",
    "            block = block,\n",
    "            dload = dload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model onto dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vrae.fit(train_dataset)\n",
    "\n",
    "#If the model has to be saved, with the learnt parameters use:\n",
    "# vrae.fit(dataset, save = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the input timeseries to encoded latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the latent vectors have to be saved, pass the parameter `save`\n",
    "z_run = vrae.transform(train_dataset, save = True)\n",
    "z_run.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model to be fetched later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vrae.save('./vrae.pth')\n",
    "\n",
    "# To load a presaved model, execute:\n",
    "# vrae.load('vrae.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some reconstructions\n",
    "reconstructions = vrae.reconstruct(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize using PCA and tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "import scipy.io\n",
    "\n",
    "z_run_tsne = TSNE(perplexity=80, min_grad_norm=1E-12, n_iter=3000).fit_transform(z_run)\n",
    "# scipy.io.savemat('tsne_vae_embeddings_20210618_Pop_Cage_001.mat', {'data': z_run_tsne.T})\n",
    "\n",
    "# plot_clustering(z_run, y_train, engine='matplotlib', download = False)\n",
    "# If plotly to be used as rendering engine, uncomment below line\n",
    "#plot_clustering(z_run, y_val, engine='plotly', download = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create clusters.annot\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# # Predict cluster assignments\n",
    "# gm = GaussianMixture(n_components=5, random_state=0).fit(z_run)\n",
    "# clusters = gm.predict(z_run)\n",
    "\n",
    "# # Number of seconds in each sequence\n",
    "# filt_time_step = 0.025\n",
    "# num_secs_seq = sequence_length * filt_time_step\n",
    "# end_time = len(z_run) * num_secs_seq + num_secs_seq\n",
    "\n",
    "# # Print head of the file\n",
    "# f = open ('Pop01-06_18_2021.annot','w')\n",
    "# # write the header--------------------\n",
    "# f.write('Bento annotation file\\n')\n",
    "# f.write('Movie file(s): {}\\n\\n'.format('Pop_20210618_cage_C1_01.avi'))\n",
    "# f.write('{0} {1}\\n'.format('Stimulus name:',''))\n",
    "# f.write('{0} {1}\\n'.format('Annotation start frame:',1))\n",
    "# f.write('{0} {1}\\n'.format('Annotation stop frame:', 26994))\n",
    "# f.write('{0} {1}\\n'.format('Annotation framerate:', 30))\n",
    "\n",
    "# f.write('\\n{0}\\n'.format('List of channels:'))\n",
    "# channels = ['cluster_num']\n",
    "# for item in channels:\n",
    "#         f.write('{0}\\n'.format(item))\n",
    "# f.write('\\n');\n",
    "\n",
    "# f.write('{0}\\n'.format('List of annotations:'))\n",
    "# clust_names = ['cluster_{}'.format(str(num)) for num in set(clusters)]\n",
    "# labels = clust_names\n",
    "# # labels = [item.replace(' ','_') for item in labels]\n",
    "# for item in labels:\n",
    "#     f.write('{0}\\n'.format(item))\n",
    "# f.write('\\n')\n",
    "\n",
    "# # now write the contents---------------\n",
    "# for ch in channels:\n",
    "#     f.write('{0}----------\\n'.format(ch))\n",
    "#     for beh in labels:\n",
    "#         f.write('>{0}\\n'.format(beh))\n",
    "#         f.write('{0}\\t {1}\\t {2} \\n'.format('Start','Stop','Duration'))\n",
    "\n",
    "#         idxs = np.where(clusters == int(beh.split('_')[-1]))[0]\n",
    "#         for hit in idxs:\n",
    "#             start_time = hit * num_secs_seq/2\n",
    "#             end_time = start_time + num_secs_seq\n",
    "#             f.write('{0}\\t{1}\\t{2}\\n'.format(start_time, end_time, num_secs_seq))\n",
    "#         f.write('\\n')\n",
    "#     f.write('\\n')\n",
    "\n",
    "# f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
